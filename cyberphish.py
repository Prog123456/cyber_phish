# -*- coding: utf-8 -*-
"""cyberphish.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Mu_BGKYSn5nrR93ksd-LAPSUjvd94JNS
"""

#importing necessary libraries

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from tensorflow import keras
from tensorflow.keras import layers
from sklearn.metrics import f1_score as f1
from sklearn.metrics import confusion_matrix,accuracy_score,recall_score,f1_score

import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)

from google.colab import drive
drive.mount('/content/drive')

data_dir = '/content/drive/My drive/SAC10495/'

#loading the dataset

df = pd.read_csv('/content/drive/My Drive/SAC10495/dataset.csv')
pd.set_option('display.max_columns', None)
df

df.info()

df['Result'].value_counts()

"""1 means legitimate, 0 is suspicious and -1 is phishing."""

df['Page_Rank'].value_counts()

"""PageRank PageRank is a value ranging from “0” to “1”. PageRank aims to measure how important a webpage is on the Internet. The greater the PageRank value the more important the webpage. In our datasets, we find that about 95% of phishing webpages have no PageRank. Moreover, we find that the remaining 5% of phishing webpages may reach a PageRank value up to “0.2”. Rule: IF {PageRank<0.2 → Phishing {Otherwise → Legitimate"""

df['Google_Index'].value_counts()

"""Google Index This feature examines whether a website is in Google’s index or not. When a site is indexed by Google, it is displayed on search results (Webmaster resources, 2014). Usually, phishing webpages are merely accessible for a short period and as a result, many phishing webpages may not be found on the Google index. Rule: IF {Webpage Indexed by Google → Legitimate {Otherwise → Phishing"""

df['URLURL_Length'].value_counts()

"""Long URL to Hide the Suspicious Part Phishers can use long URL to hide the doubtful part in the address bar. For example: http://federmacedoadv.com.br/3f/aze/ab51e2e319e51502f416dbe46b773a5e/?cmd=_home&amp;dispatch=11004d58f5b74f8dc1e7c2e8dd4105e811004d58f5b74f8dc1e7c2e8dd4105e8@phishing.website.html To ensure accuracy of our study, we calculated the length of URLs in the dataset and produced an average URL length. The results showed that if the length of the URL is greater than or equal 54 characters then the URL classified as phishing. By reviewing our dataset we were able to find 1220 URLs lengths equals to 54 or more which constitute 48.8% of the total dataset size. We have been able to update this feature rule by using a method based on frequency and thus improving upon its accuracy. RULE: IF {URL length<54 → feature = Legitimate {else if URL length≥54 and ≤75 → feature = Suspicious {otherwise→ feature = Phishing"""

df['having_At_Symbol'].value_counts()

"""URL’s having “@” Symbol Using “@” symbol in the URL leads the browser to ignore everything preceding the “@” symbol and the real address often follows the “@” symbol. RULE: IF {Url Having @ Symbol→ Phishing {Otherwise→ Legitimate


"""

df['double_slash_redirecting'].value_counts()

"""Redirecting using “//” The existence of “//” within the URL path means that the user will be redirected to another website. An example of such URL’s is: “http://www.legitimate.com//http://www.phishing.com”. We examin the location where the “//” appears. We find that if the URL starts with “HTTP”, that means the “//” should appear in the sixth position. However, if the URL employs “HTTPS” then the “//” should appear in seventh position. RULE: IF {The Position of the Last Occurrence of "//\" " in the URL > 7→ Phishing {Otherwise→ Legitimate}"""

df['Prefix_Suffix'].value_counts()

"""Adding Prefix or Suffix Separated by (-) to the Domain The dash symbol is rarely used in legitimate URLs. Phishers tend to add prefixes or suffixes separated by (-) to the domain name so that users feel that they are dealing with a legitimate webpage. For example http://www.Confirme-paypal.com/. RULE: IF {Domain Name Part Includes (-) Symbol → Phishing {Otherwise → Legitimate"""

df.columns

selected_features=['URLURL_Length', 'having_At_Symbol','double_slash_redirecting', 'Prefix_Suffix', 'Page_Rank', 'Google_Index','Result']
df2 = df[selected_features]
df2.head()

new_columns = {'URLURL_Length': 'URL_Length', 'Prefix_Suffix':'HavingHyphen'}
df2= df2.rename(columns=new_columns)

df2 = df2.drop_duplicates()
df2.shape

"""There are total 107 different combinations, this may sounds really less but keep in mind that all the features and labels has combination of only two or three unique values.

The less number will generalize more but detection time will be alot faster.
"""

df2.isna().sum()

df2['Result'].value_counts(normalize=True)

"""1 means legitimate, and -1 is phishing."""

X = df2.drop('Result', axis=1)
y = df2['Result']

#@title Feature Extraction of Variational Auto Encoder Model

"""Autoencoder is a type of neural network that can be used to learn a compressed representation of raw data.

An autoencoder is composed of an encoder and a decoder sub-models. The encoder compresses the input and the decoder attempts to recreate the input from the compressed version provided by the encoder. After training, the encoder model is saved and the decoder is discarded.

The encoder can then be used as a data preparation technique to perform feature extraction on raw data that can be used to train a different machine learning model.
"""

print(X.shape, y.shape)

from sklearn.preprocessing import MinMaxScaler
# split into train test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)
# scale data
t = MinMaxScaler()
t.fit(X_train)
X_train = t.transform(X_train)
X_test = t.transform(X_test)

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import LeakyReLU
from tensorflow.keras.layers import BatchNormalization
from tensorflow.keras.utils import plot_model
n_inputs=X_train.shape[1]
# define encoder
visible = Input(shape=(n_inputs,))
# encoder level 1
e = Dense(n_inputs*2)(visible)
e = BatchNormalization()(e)
e = LeakyReLU()(e)
# encoder level 2
e = Dense(n_inputs)(e)
e = BatchNormalization()(e)
e = LeakyReLU()(e)
# bottleneck
n_bottleneck = n_inputs
bottleneck = Dense(n_bottleneck)(e)

# define decoder, level 1
d = Dense(n_inputs)(bottleneck)
d = BatchNormalization()(d)
d = LeakyReLU()(d)
# decoder level 2
d = Dense(n_inputs*2)(d)
d = BatchNormalization()(d)
d = LeakyReLU()(d)

# output layer
output = Dense(n_inputs, activation='linear')(d)

# define autoencoder model
model = Model(inputs=visible, outputs=output)

# compile autoencoder model
model.compile(optimizer='adam', loss='mse')

# plot the autoencoder
plot_model(model, 'autoencoder_no_compress.png', show_shapes=True)

"""The image above shows a plot of the autoencoder.

We will define the encoder to have two hidden layers, the first with two times the number of inputs 6 and the second with the same number of inputs (12), followed by the bottleneck layer with the same number of inputs as the dataset (6).

The decoder will be defined with a similar structure, although in reverse.

It will have two hidden layers, the first with the number of inputs in the dataset (6) and the second with double the number of inputs (12). The output layer will have the same number of nodes as there are columns in the input data and will use a linear activation function to output numeric values.
"""

# fit the autoencoder model to reconstruct input
history = model.fit(X_train, X_train, epochs=200, batch_size=32, verbose=2, validation_data=(X_test,X_test))

# plot loss
plt.plot(history.history['loss'], label='train',color='blue')
plt.plot(history.history['val_loss'], label='test',color='red')
plt.xlabel('Epochs',fontweight='bold')
plt.ylabel('Loss',fontweight='bold')
plt.title('Training and Testing Loss for Variational Auto Encoder',fontweight='bold')
plt.legend()
plt.show()

# define an encoder model (without the decoder)
encoder = Model(inputs=visible, outputs=bottleneck)
plot_model(encoder, 'encoder_no_compress.png', show_shapes=True)

# save the encoder model to drive
encoder.save('/content/drive/My Drive/SAC10495/encoder_model.h5')

"""So far, so good. We know how to develop an autoencoder without compression.

Next, let’s change the configuration of the model so that the bottleneck layer has half the number of nodes
"""

# bottleneck
n_bottleneck = round(float(n_inputs) / 2.0)
bottleneck = Dense(n_bottleneck)(e)

# define decoder, level 1
d = Dense(n_inputs)(bottleneck)
d = BatchNormalization()(d)
d = LeakyReLU()(d)
# decoder level 2
d = Dense(n_inputs*2)(d)
d = BatchNormalization()(d)
d = LeakyReLU()(d)
# output layer
output = Dense(n_inputs, activation='linear')(d)
# define autoencoder model
model = Model(inputs=visible, outputs=output)
# compile autoencoder model
model.compile(optimizer='adam', loss='mse')

# plot the autoencoder
plot_model(model, 'autoencoder_compress.png', show_shapes=True)

# fit the autoencoder model to reconstruct input
history = model.fit(X_train, X_train, epochs=200, batch_size=32, verbose=2, validation_data=(X_test,X_test))

# plot loss
plt.plot(history.history['loss'], label='train',color='blue')
plt.plot(history.history['val_loss'], label='test',color='red')
plt.xlabel('Epochs',fontweight='bold')
plt.ylabel('Loss',fontweight='bold')
plt.title('Training and Testing Loss for Variational Auto Encoder after Reconstruction',fontweight='bold')
plt.legend()
plt.show()

# define an encoder model (without the decoder)
encoder_reconstructed = Model(inputs=visible, outputs=bottleneck)
plot_model(encoder_reconstructed, 'encoder_compress.png', show_shapes=True)
# save the encoder model to drive
encoder_reconstructed.save('/content/drive/My Drive/SAC10495/encoder_reconstructed_model.h5')

#@title Feature Extraction by Darknet19 Model

X_train.shape[1]

import keras
from keras.layers import Conv2D, Input, concatenate
from keras.layers import LeakyReLU, MaxPooling2D, BatchNormalization,GlobalAveragePooling2D
from keras.models import Model
from keras.activations import softmax
from functools import partial

from keras.layers import Input, Conv2D, LeakyReLU, BatchNormalization, MaxPooling2D, GlobalAveragePooling2D, Lambda
from keras.models import Model
from keras.activations import softmax
from functools import partial

new_conv = partial(Conv2D, padding="same")

def _base_block(out, x):
    "(3,3), Leaky, Batch"
    x = new_conv(out, (3,3))(x)
    x = LeakyReLU(alpha=0.1)(x)
    x = BatchNormalization()(x)
    return x

def _block_1(out, x):
    """
    output follows:
    out//2, out
    """
    x = new_conv(out//2, (1,1))(x)
    x = LeakyReLU(alpha=0.1)(x)
    x = BatchNormalization()(x)
    x = _base_block(out, x)
    return x

def _block_2(out, x):
    """
    output follows:
    out, out//2, out
    """
    x = _base_block(out, x)
    x = _block_1(out, x)
    return x

def Darknet19():
    input_layer = Input((6, 6, 3))  # Adjusted input shape
    x = _base_block(32, input_layer)
    x = _base_block(64, x)
    x = _block_2(128, x)
    x = _block_2(256, x)
    x = _block_2(512, x)
    x = _block_1(512, x)
    x = _block_2(1024, x)
    x = _block_1(512, x)
    x = new_conv(1, (1,1), activation="linear")(x)
    model = Model(inputs=input_layer, outputs=x)
    return model

def apply_soft(x):
    output = softmax(x)
    return output

def Darknet_classifier():
    base_model = Darknet19()
    x = base_model.output
    x = GlobalAveragePooling2D()(x)
    output = Lambda(apply_soft)(x)
    model = Model(inputs=base_model.inputs, outputs=output)
    return model

img_size = X_train.shape[1]
darknet_model = Darknet_classifier()
print(darknet_model.summary())

print('Input layer of X_train.shape[1]:',X_train.shape[1])

#plot the darknet19 model
# Generate the plot
plot_model(Darknet_classifier(), to_file='darknet_classifier_plot.png', show_shapes=True, show_layer_names=True)

# Save the model
darknet_classifier_model = Darknet_classifier()
darknet_classifier_model.save('/content/drive/MyDrive/SAC10495/darknet_classifier_model.h5')

from tensorflow.keras import models, optimizers

# Train your model using your data
darknet_classifier_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
history_darknet=darknet_classifier_model.fit(X_train, X_train, epochs=5, batch_size=32, validation_data=(X_test, X_test))

# Evaluate the model
loss, accuracy = darknet_model.evaluate(X_test, y_test)
print(f'Test accuracy: {accuracy}')

history_darknet = darknet_classifier_model.fit(X_train, X_train, epochs=200, batch_size=32, verbose=2, validation_data=(X_test,X_test))

